name: Integration Tests

on:
    push:
        branches: [main, staging]
    pull_request:
        branches: [main, staging]

jobs:
    integration-test:
        runs-on: ubuntu-latest

        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Docker Buildx
              uses: docker/setup-buildx-action@v3

            # Cache Docker layers for your application images
            - name: Cache Docker layers
              uses: actions/cache@v4
              with:
                  path: /tmp/.buildx-cache
                  key: ${{ runner.os }}-buildx-${{ github.sha }}
                  restore-keys: |
                      ${{ runner.os }}-buildx-

            # Cache all Docker images together
            - name: Cache Docker images
              id: cache-images
              uses: actions/cache@v4
              with:
                  path: /tmp/docker-images.tar
                  key: docker-images-redis7-ollama-v1
                  restore-keys: |
                      docker-images-

            # Pull and save all images if not cached
            - name: Pull and save Docker images
              if: steps.cache-images.outputs.cache-hit != 'true'
              run: |
                  echo "Pulling Docker images..."
                  docker pull redis:7-alpine
                  docker pull ollama/ollama:latest
                  echo "Saving images to cache..."
                  docker save redis:7-alpine ollama/ollama:latest -o /tmp/docker-images.tar

            # Load images from cache
            - name: Load Docker images
              if: steps.cache-images.outputs.cache-hit == 'true'
              run: |
                  echo "Loading images from cache..."
                  docker load -i /tmp/docker-images.tar

            # Cache the Ollama model separately (this is the biggest time saver)
            - name: Cache Ollama model
              id: cache-model
              uses: actions/cache@v4
              with:
                  path: /tmp/ollama-models
                  key: ollama-model-qwen2.5-0.5b-v1
                  restore-keys: |
                      ollama-model-qwen2.5-0.5b-

            - name: Start Ollama container with model cache
              run: |
                  # Create model directory if it doesn't exist
                  mkdir -p /tmp/ollama-models

                  # Start Ollama with volume mount for model persistence
                  docker run -d \
                    --name ollama-test \
                    -p 11434:11434 \
                    -v /tmp/ollama-models:/root/.ollama \
                    ollama/ollama:latest

                  # Wait for Ollama to be ready
                  echo "Waiting for Ollama to start..."
                  timeout 120 bash -c 'until curl -f http://localhost:11434/api/version; do sleep 5; done'
                  echo "Ollama is ready"

            # Only pull model if not cached
            - name: Pull qwen2.5:0.5b model
              if: steps.cache-model.outputs.cache-hit != 'true'
              run: |
                  echo "Pulling qwen2.5:0.5b model..."
                  curl -X POST -H "Content-Type: application/json" -d '{"model": "qwen2.5:0.5b"}' http://localhost:11434/api/pull

                  # Wait for pull to complete
                  timeout 300 bash -c 'while [ "$(curl -s -X POST -H "Content-Type: application/json" -d "{\"model\": \"qwen2.5:0.5b\"}" http://localhost:11434/api/pull | jq -r .status 2>/dev/null || echo "pulling")" = "pulling" ]; do echo "Still pulling..."; sleep 10; done'
                  echo "Model qwen2.5:0.5b is ready"

            # Verify model is available (for both cached and fresh installs)
            - name: Verify model availability
              run: |
                  echo "Verifying model is available..."
                  curl -X POST -H "Content-Type: application/json" -d '{"model": "qwen2.5:0.5b"}' http://localhost:11434/api/show

            - name: Create .env file for testing
              run: |
                  cat > .env << EOF
                  NODE_ENV=test
                  SERVER_PORT=4000
                  CLIENT_PORT=3000
                  REDIS_PORT=6379
                  REDIS_PASSWORD=
                  REDIS_DB=0
                  REDIS_KEY_PREFIX=GridLLM:
                  WORKER_ID=worker-ci-001
                  OLLAMA_HOST=ollama-test
                  OLLAMA_PORT=11434
                  OLLAMA_PROTOCOL=http
                  API_KEY=test-api-key
                  LOG_LEVEL=info
                  EOF

            - name: Build Docker images with cache
              run: |
                  docker compose build --no-cache \
                    --cache-from type=local,src=/tmp/.buildx-cache \
                    --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max

            # Move cache to avoid cache bloat
            - name: Move cache
              run: |
                  rm -rf /tmp/.buildx-cache
                  mv /tmp/.buildx-cache-new /tmp/.buildx-cache

            - name: Start services and connect Ollama
              run: |
                  # Start compose services first to create the network
                  npm run bundle:full

                  # Find the compose network and connect Ollama to it
                  COMPOSE_NETWORK=$(docker network ls --filter name=gridllm --format "{{.Name}}" | head -1)
                  echo "Connecting ollama-test to compose network: $COMPOSE_NETWORK"
                  docker network connect "$COMPOSE_NETWORK" ollama-test

                  # Verify connection
                  echo "Networks for ollama-test:"
                  docker inspect ollama-test --format='{{range $k, $v := .NetworkSettings.Networks}}{{$k}} {{end}}'

            - name: Wait for services to be healthy
              run: |
                  echo "Waiting for services to start..."
                  sleep 30

                  # Wait for Redis to be healthy
                  timeout 30 bash -c 'until docker compose exec -T redis redis-cli ping; do sleep 2; done'
                  echo "Redis is healthy"

                  # Wait for Server to be healthy
                  timeout 30 bash -c 'until curl -f http://localhost:4000/health; do sleep 5; done'
                  echo "Server is healthy"

                  # Wait for Client to be healthy
                  timeout 30 bash -c 'until curl -f http://localhost:3000/health; do sleep 5; done'
                  echo "Client is healthy"

            - name: Check if services are healthy
              run: |
                  echo "Testing server health endpoint..."
                  curl -f http://localhost:4000/health

                  echo "Testing client health endpoint..."
                  curl -f http://localhost:3000/health

                  echo "Testing Redis connection..."
                  docker compose exec -T redis redis-cli ping

                  echo "Testing Ollama connection..."
                  curl -f http://localhost:11434/api/version

                  echo "All services are running and healthy!"

            - name: Test /ollama Endpoints
              run: |
                  echo "Testing POST /ollama/api/generate"
                  curl -f -X POST -H "Content-Type: application/json" -d '{"prompt": "What country is New York in? Answer in just the country.", "model": "qwen2.5:0.5b"}' http://localhost:4000/ollama/api/generate

            - name: Show service logs on failure
              if: failure()
              run: |
                  echo "=== Server logs ==="
                  docker compose logs server
                  echo "=== Client logs ==="
                  docker compose logs client
                  echo "=== Redis logs ==="
                  docker compose logs redis
                  echo "=== Ollama logs ==="
                  docker logs ollama-test 2>/dev/null || echo "Ollama container not running"

            - name: Stop and cleanup
              if: always()
              run: |
                  docker compose down -v
                  docker stop ollama-test 2>/dev/null || true
                  docker rm ollama-test 2>/dev/null || true
                  # Don't prune everything to preserve caches
                  docker container prune -f
